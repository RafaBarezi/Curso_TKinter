{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN72E1Y8RcHTr0YcK89KWmI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Configuração do ambiente"
      ],
      "metadata": {
        "id": "l9qg7Ny280Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "h-NsygwWgcHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "7ZJO4DVWgexx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "P7tSXpdXgiCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "AhObXfYMgoZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "TcXQftFqgqzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6419f086-c9ac-45e9-96c6-8fcf8b5db3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurar Variáveis de Ambiente"
      ],
      "metadata": {
        "id": "0jYHvZofFQjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "HtvgG8gfguM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniciar o pacote"
      ],
      "metadata": {
        "id": "mN5uume9DEDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xCbE18lyzMpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark  - Instanciar o SparkContext"
      ],
      "metadata": {
        "id": "mueyuPkzFXdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext() # Instantiate SparkContext"
      ],
      "metadata": {
        "id": "nEk5L4Ee5KAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_contexto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNHz2oC4gcrx",
        "outputId": "423735d1-6a43-49e5-b616-4bc5cef766f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Zxxz8_tkgv8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "tjPpLCjl3PRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformando o python em RDD do spark\n",
        "paralelo = spark_contexto.parallelize(vetor)\n",
        "print(paralelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMRJY-Kj3VCT",
        "outputId": "56910085-274b-4d51-e265-21391224000c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x**2+x) # x²+x"
      ],
      "metadata": {
        "id": "CTNWvnjg4wUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a8tWh541ir",
        "outputId": "4691319a-6630-4c61-a9a7-91d47729c006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize([\"distribuida\", \"distribuida\", \"spark\", \"rdd\", \"spark\", \"spark\"])"
      ],
      "metadata": {
        "id": "lFY8XDHw5B4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "funcao_lambda = lambda x:(x,1)"
      ],
      "metadata": {
        "id": "V8h3zdh15Gse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()"
      ],
      "metadata": {
        "id": "U-71hjwO5Lvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (w, c) in mapa:\n",
        "\n",
        "  print(\"{}: {}\".format(w, c))\n",
        "\n",
        "# Esse código percorre a lista mapa e imprime cada par formado pela palavra e sua respectiva ocorrência. A saída é a seguinte:\n",
        "\n",
        "distribuida: 2\n",
        "\n",
        "spark: 3\n",
        "\n",
        "rdd: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXeCzDAV5QWc",
        "outputId": "9c9bbf60-901f-48f4-d2af-0c6220d3bb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribuida: 2\n",
            "spark: 3\n",
            "rdd: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "TxGee2-o5dLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}